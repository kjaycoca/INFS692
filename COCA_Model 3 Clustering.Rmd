---
title: "Data Science Final Project"
subtitle: "MODEL 3: CLUSTERING TECHNIQUE"
author: "Kjay O. Coca"
date: "2022-12-14"
output: pdf_document
---


# Helper Packages And Modeling Packages
```{r}
library(dplyr)    
library(ggplot2)   
library(stringr)  
library(cluster)    
library(factoextra)
library(gridExtra)  
library(tidyverse)
library(readr)
library(mclust)
```

## IMPORTING THE DATA
```{r}
set.seed(123)
radiomics_data <- read_csv("D:/1 MASTERS/STAT225/FINAL PROJECT/STAT 325 _FINAL PROJECT_/Normalize Radiomics Data.csv")
```

Below are the first 6 of the data.
```{r}
head(radiomics_data)
```

## scaling/standardizing the data
```{r}
radiomics_data <- radiomics_data[c(3:431)]
```


```{r}
sum(is.na(radiomics_data))
```
Since there is no N/A's, we can proceed with the clustering.


#CLUSTERS

## K means clustering
```{r}
kmeans(radiomics_data, centers = 3, iter.max = 100, nstart = 100)
clusters <- kmeans(radiomics_data, centers = 3, iter.max = 100, nstart = 100)
```
Clustering using kmeans has 41.9 percent with k = 3.


## K means clustering using WSS
```{r}
fviz_nbclust(radiomics_data, kmeans, method = "wss") 
```
Using elbow method to identify k, k=2.


## K means clustering using silhouette
```{r}
fviz_nbclust(radiomics_data, kmeans, method = "silhouette")
```
Silhoutte suggest that number of cluster is 2.

## K means clustering using gap_stat
```{r}
fviz_nbclust(radiomics_data, kmeans, method = "gap_stat") 
```
Gap-stat suggest that the number of cluster is 2.


Since cluster is equal to 2, the number of cluster is 2. Below is the cluster using 2 cluster.
```{r}
clusters <- kmeans(radiomics_data, centers = 2, iter.max = 100, nstart = 100)
```

```{r}
fviz_cluster(kmeans(radiomics_data, centers = 2, iter.max = 100, nstart = 100), data = radiomics_data)
```
Above image shows the cluster structure using 2 number of clusters.


## The quality of a k-means partition.
```{r}
clusters$betweenss / clusters$totss
```
The quality of k means partition is 0.4192 or 41.92 percent.


# 2. Heirarchical Clustering
```{r}
radiomics_data <- radiomics_data %>%
  select_if(is.numeric) %>%  # select numeric columns
  mutate_all(as.double) %>%  # coerce to double type
  scale()
d <- dist(radiomics_data, method = "euclidean")
```


## Hierarchical clustering using Complete Linkage
```{r}
hc1 <- hclust(d, method = "complete")
plot(hc1, cex = 0.6)
rect.hclust(hc1, k = 2, border = 1:4)
```

## Hierarchical clustering using AGNES
```{r}
set.seed(123)
hc2 <- agnes(radiomics_data, method = "complete")
hc2$ac
```
Using the AGNES, it has 80.77 percent quality of partition.

## Hierarchical clustering using DIANA
```{r}
hc4 <- diana(radiomics_data)
hc4$dc
```
Using the DIANA, it has 79.19 percent quality of partition.


```{r}
wss_plot <- fviz_nbclust(radiomics_data, FUN = hcut, method = "wss", 
                   k.max = 10) +
  ggtitle("(A) Elbow method")
silhouette_plot <- fviz_nbclust(radiomics_data, FUN = hcut, method = "silhouette", 
                   k.max = 10) +
  ggtitle("(B) Silhouette method")
gapstat_plot <- fviz_nbclust(radiomics_data, FUN = hcut, method = "gap_stat", 
                   k.max = 10) +
  ggtitle("(C) Gap statistic")
```

```{r}
gridExtra::grid.arrange(wss_plot, silhouette_plot, gapstat_plot, nrow = 1)
```
The result of the graph shows that they give different number of cluster in each method. Elbow method gives 3, silhouette gives 2 and gap stat gives 9. The difference between heirarchical and k means is that k means gives the same number of cluster in gap stat, silhouette and elbow method. While in heirarchical does not.


## Ward's method
```{r}
hc5 <- hclust(d, method = "ward.D2" )
sub_grp <- cutree(hc5, k = 9)
table(sub_grp)
```
Above is the number of sample in each cluster. If number of cluster is 8, the member of cluster is 22.


## 3. Model Based
```{r}
set.seed(123)
model_based3 <- Mclust(radiomics_data[,1:10], G=3) 
summary(model_based3)
```
The result shows 3 optimal number of clusters with BIC -2632.206. A negative zone with the highest value indicates the preferred model, In general, the lower the BIC value, the better. Plot the results with BIC, density and uncertainty.


```{r}
set.seed(123)
model_based4 = Mclust(radiomics_data, 1:9) 
summary(model_based4)
```
The result shows 3 optimal number of clusters with BIC -178301. A negative zone with the highest value indicates the preferred model, In general, the lower the BIC value, the better. Plot the results with BIC, density and uncertainty.


## Plot results
```{r}
plot(model_based3, what = "density")
plot(model_based3, what = "uncertainty")
```


## Observations with high uncertainty
```{r}
sort(model_based3$uncertainty, decreasing = TRUE) %>% head()
```

```{r}
legend_args <- list(x = "bottomright", ncol = 5)
plot(model_based3, what = 'BIC', legendArgs = legend_args)
plot(model_based3, what = 'classification')
plot(model_based3, what = 'uncertainty')
```

```{r}
probabilities <- model_based3$z 
colnames(probabilities) <- paste0('C', 1:3)
```

```{r}
probabilities <- probabilities %>%
  as.data.frame() %>%
  mutate(id = row_number()) %>%
  tidyr::gather(cluster, probability, -id)
```
Plot the observations that are aligned to each cluster but their uncertainty of membership is greater than 0.25.

```{r}
ggplot(probabilities, aes(probability)) +
  geom_histogram() +
  facet_wrap(~ cluster, nrow = 2)
```

```{r}
uncertainty <- data.frame(
  id = 1:nrow(radiomics_data),
  cluster = model_based3$classification,
  uncertainty = model_based3$uncertainty
)
```
Plot the average standardized consumption for cluster 2 observations compared to all observations.


```{r}
uncertainty %>%
  group_by(cluster) %>%
  filter(uncertainty > 0.25) %>%
  ggplot(aes(uncertainty, reorder(id, uncertainty))) +
  geom_point() +
  facet_wrap(~ cluster, scales = 'free_y', nrow = 1)
```

```{r}
cluster2 <- radiomics_data %>%
  scale() %>%
  as.data.frame() %>%
  mutate(cluster = model_based3$classification) %>%
  filter(cluster == 2) %>%
  select(-cluster)
```

```{r}
cluster2 %>%
  tidyr::gather(product, std_count) %>%
  group_by(product) %>%
  summarize(avg = mean(std_count)) %>%
  ggplot(aes(avg, reorder(product, avg))) +
  geom_point() +
  labs(x = "Average standardized consumption", y = NULL)

```


The advantage of model-based clustering over K-means and hierarchical clustering is that it automatically determines the ideal number of clusters. In this clustering, Gaussian mixture models is applied, which are one of the most popular model-based clustering approaches available. Using df values in k-means clustering since it is already standardized, we can use Mclust() function. Leaving G = NULL forces Mclust() to evaluate 1â€“9 clusters and select the optimal number of components based on BIC.


